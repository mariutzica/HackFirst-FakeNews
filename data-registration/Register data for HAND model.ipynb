{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites: python 3.6 or later\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import uuid\n",
    "import pprint\n",
    "import datetime\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is from the nb provided by Dan Feldman; helper functions to assist in registering datasets and resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a convenience method to handle api responses. The main portion of the notebook starts in the the next cell\n",
    "def handle_api_response(response, print_response=False):\n",
    "    parsed_response = response.json()\n",
    "\n",
    "    if print_response:\n",
    "        pp.pprint({\"API Response\": parsed_response})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return parsed_response\n",
    "    elif response.status_code == 400:\n",
    "        raise Exception(\"Bad request ^\")\n",
    "    elif response.status_code == 403:\n",
    "        msg = \"Please make sure your request headers include X-Api-Key and that you are using correct url\"\n",
    "        raise Exception(msg)\n",
    "    else:\n",
    "        now = datetime.datetime.utcnow().replace(microsecond=0).isoformat()\n",
    "        msg = f\"\"\"\\n\\n\n",
    "        ------------------------------------- BEGIN ERROR MESSAGE -----------------------------------------\n",
    "        It seems our server encountered an error which it doesn't know how to handle yet. \n",
    "        This sometimes happens with unexpected input(s). In order to help us diagnose and resolve the issue, \n",
    "        could you please fill out the following information and email the entire message between ----- to\n",
    "        danf@usc.edu:\n",
    "        1) URL of notebook (of using the one from https://hub.mybinder.org/...): [*****PLEASE INSERT ONE HERE*****]\n",
    "        2) Snapshot/picture of the cell that resulted in this error: [*****PLEASE INSERT ONE HERE*****]\n",
    "        \n",
    "        Thank you and we apologize for any inconvenience. We'll get back to you as soon as possible!\n",
    "        \n",
    "        Sincerely, \n",
    "        Dan Feldman\n",
    "        \n",
    "        Automatically generated summary:\n",
    "        - Time of occurrence: {now}\n",
    "        - Request method + url: {response.request.method} - {response.request.url}\n",
    "        - Request headers: {response.request.headers}\n",
    "        - Request body: {response.request.body}\n",
    "        - Response: {parsed_response}\n",
    "\n",
    "        --------------------------------------- END ERROR MESSAGE ------------------------------------------\n",
    "        \\n\\n\n",
    "        \"\"\"\n",
    "\n",
    "        raise Exception(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For real interactions with the data catalog, use api.mint-data-catalog.org\n",
    "url = \"https://sandbox.mint-data-catalog.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you register datasets or resources, we require you to pass a \"provenance_id\". This a unique id associated\n",
    "# with your account so that we can keep track of who is adding things to the data catalog. For sandboxed interactions\n",
    "# with the data catalog api, please use this provenance_id:\n",
    "provenance_id = \"e8287ea4-e6f2-47aa-8bfc-0c22852735c8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X-Api-Key': 'mint-data-catalog:96c4d87b-3551-4d95-bf84-96dd86f07a8e:1ec0107c-295c-4404-8254-2355411e0264'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get session token to use the API\n",
    "resp = requests.get(f\"{url}/get_session_token\").json()\n",
    "print(resp)\n",
    "api_key = resp['X-Api-Key']\n",
    "\n",
    "request_headers = {\n",
    "    'Content-Type': \"application/json\",\n",
    "    'X-Api-Key': api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Dan's example to check for the streamflow variable from svo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This name does not exist: bd6d7051-4788-427a-9c91-f892ff4c5aa0\n",
      "{ 'API Response': { 'result': 'success',\n",
      "                    'standard_variables': [ { 'description': '',\n",
      "                                              'id': '28f016f3-60fc-524c-a888-b40bc694bfb6',\n",
      "                                              'name': 'channel_water_x-section__volume_flow_rate',\n",
      "                                              'ontology': 'SVOv1.0',\n",
      "                                              'uri': 'http://geoscienceontology.org/svo/svl/variable#channel%40context%257Ein_%28water_flowing_x-section%29__volume_flow_rate'},\n",
      "                                            { 'description': '',\n",
      "                                              'id': 'eeaa3017-a87f-5887-a6cd-a4e196aa8df4',\n",
      "                                              'name': 'Time_Standard_Variable',\n",
      "                                              'ontology': 'MyOntology',\n",
      "                                              'uri': 'http://my_ontology_uri.org/standard_names/time_standard_variable'}]}}\n"
     ]
    }
   ],
   "source": [
    "# If you need to check if specific standard variables have already been registered in the data catalog, \n",
    "# you can search by name and data catalog will return existing records.\n",
    "nonexistent_name = str(uuid.uuid4())\n",
    "print(f\"This name does not exist: {nonexistent_name}\")\n",
    "\n",
    "search_query = {\n",
    "    \"name__in\": [\"Time_Standard_Variable\", \"channel_water_x-section__volume_flow_rate\", nonexistent_name]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/knowledge_graph/find_standard_variables\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=search_query)\n",
    "parsed_response = handle_api_response(resp, print_response=True)\n",
    "\n",
    "# Below is how you'd extract standard_variables from the response if you need to reference them (their record_ids)\n",
    "# later:\n",
    "# \n",
    "# existing_standard_variables = parsed_response[\"standard_variables\"]\n",
    "# print(existing_standard_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables are not present so need to register them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'API Response': { 'result': 'success',\n",
      "                    'standard_variables': [ { 'description': '',\n",
      "                                              'name': 'gage__observation_time',\n",
      "                                              'ontology': 'SVOv1.0',\n",
      "                                              'record_id': 'c8b8a9f6-9288-551c-97dc-9be72e1c6eba',\n",
      "                                              'uri': 'http://geoscienceontology.org/svo/svl/variable#gage%40context%257Eat_observation__observation_time'},\n",
      "                                            { 'description': '',\n",
      "                                              'name': 'channel_water_x-section__volume_flow_rate',\n",
      "                                              'ontology': 'SVOv1.0',\n",
      "                                              'record_id': '28f016f3-60fc-524c-a888-b40bc694bfb6',\n",
      "                                              'uri': 'http://geoscienceontology.org/svo/svl/variable#channel%40context%257Ein_%28water_flowing_x-section%29__volume_flow_rate'},\n",
      "                                            { 'description': '',\n",
      "                                              'name': 'gage_water_surface__height',\n",
      "                                              'ontology': 'SVOv1.0',\n",
      "                                              'record_id': '26b3f0ff-c0b3-574a-a093-bf92612edf73',\n",
      "                                              'uri': 'http://geoscienceontology.org/svo/svl/variable#gage%40context%257Eat_%28water_surface%29__height'}]}}\n"
     ]
    }
   ],
   "source": [
    "# @param[name] standard variable name (aka label)\n",
    "# @param[ontology] name of the ontology where standard variables are defined\n",
    "# @param[uri] uri of standard variable name (note that this is full uri, which includes the ontology)\n",
    "standard_variable_defs = {\n",
    "    \"standard_variables\": [\n",
    "        {\n",
    "            \"name\": \"gage__observation_time\",\n",
    "            \"ontology\": \"SVOv1.0\",\n",
    "            \"uri\": \"http://geoscienceontology.org/svo/svl/variable#gage%40context%257Eat_observation__observation_time\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"channel_water_x-section__volume_flow_rate\",\n",
    "            \"ontology\": \"SVOv1.0\",\n",
    "            \"uri\": \"http://geoscienceontology.org/svo/svl/variable#channel%40context%257Ein_%28water_flowing_x-section%29__volume_flow_rate\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gage_water_surface__height\",\n",
    "            \"ontology\": \"SVOv1.0\",\n",
    "            \"uri\": \"http://geoscienceontology.org/svo/svl/variable#gage%40context%257Eat_%28water_surface%29__height\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/knowledge_graph/register_standard_variables\", \n",
    "                    headers=request_headers, \n",
    "                    json=standard_variable_defs)\n",
    "\n",
    "\n",
    "# If request is successful, it will return 'result': 'success' along with a list of registered standard variables\n",
    "# and their record_ids. Those record_ids are unique identifiers (UUID) and you will need them down the road to \n",
    "# register variables\n",
    "parsed_response = handle_api_response(resp, print_response=True)\n",
    "records = parsed_response['standard_variables']\n",
    "\n",
    "# iterate through the list of returned standard variable objects and save\n",
    "# the ones whose names match the one that we want and store them in python variables\n",
    "time_standard_variable = next(record for record in records if record[\"name\"] == \"gage__observation_time\")\n",
    "streamflow_standard_variable = next(record for record in records if record[\"name\"] == \"channel_water_x-section__volume_flow_rate\")\n",
    "gageheight_standard_variable = next(record for record in records if record[\"name\"] == \"gage_water_surface__height\")\n",
    "\n",
    "## Uncomment below to see the structure of a specific variable:\n",
    "# pp.pprint({\"Time Standard Variable\": time_standard_variable})\n",
    "# pp.pprint({\"Temperature Standard Variable\": temperature_standard_variable})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start dataset & resource registration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"4e8ade31-7729-4891-a462-2dac66158512\" # This is optional; if not given, it will be auto-generated\n",
    "\n",
    "## An example of how to generate a random uuid yourself (will be different every time method is run)\n",
    "# print(str(uuid.uuid4()))\n",
    "# print(str(uuid.uuid4()))\n",
    "#\n",
    "## This will generate the same record_id as long as the input string remains the same\n",
    "#\n",
    "# input_string = \"some string 34_\"\n",
    "# print(str(uuid.uuid5(uuid.NAMESPACE_URL, str(input_string))))\n",
    "# print(str(uuid.uuid5(uuid.NAMESPACE_URL, str(input_string))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'API Response': { 'datasets': [ { 'description': 'USGS Streamflow data at '\n",
      "                                                   'gages in the Boulder Creek '\n",
      "                                                   'Watershed',\n",
      "                                    'json_metadata': {},\n",
      "                                    'name': 'USGS Streamflow Boulder',\n",
      "                                    'provenance_id': 'e8287ea4-e6f2-47aa-8bfc-0c22852735c8',\n",
      "                                    'record_id': '4e8ade31-7729-4891-a462-2dac66158512'}],\n",
      "                    'result': 'success'}}\n"
     ]
    }
   ],
   "source": [
    "dataset_defs = {\n",
    "    \"datasets\": [\n",
    "        {\n",
    "            \"record_id\": dataset_id, # Remove this line if you want to create a new dataset\n",
    "            \"provenance_id\": provenance_id,\n",
    "            ##\"metadata\": {\n",
    "            ##    \"any_additional_metadata\": \"content\"\n",
    "            ##},\n",
    "            \"description\": \"USGS Streamflow data at gages in the Boulder Creek Watershed\",\n",
    "            \"name\": \"USGS Streamflow Boulder\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/datasets/register_datasets\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=dataset_defs)\n",
    "\n",
    "\n",
    "parsed_response = handle_api_response(resp, print_response=True)\n",
    "\n",
    "datasets = parsed_response[\"datasets\"]\n",
    "\n",
    "# Iterate through the list of returned datasets objects and save the one whose name matches our name \n",
    "# to a Python variable\n",
    "dataset_record = next(record for record in datasets if record[\"name\"] == \"USGS Streamflow Boulder\")\n",
    "# Extract dataset record_id and store it in a variable\n",
    "dataset_record_id = dataset_record[\"record_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one time uuid generator\n",
    "#print(str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the correct units for each variable associated with the dataset. The \"Name\" attribute is the name of the variable in the file. The \"standard variable id\" is the id in SVO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'API Response': { 'result': 'success',\n",
      "                    'variables': [ { 'dataset_id': '4e8ade31-7729-4891-a462-2dac66158512',\n",
      "                                     'json_metadata': { 'units': 'ISO8601_datetime'},\n",
      "                                     'name': 'Time',\n",
      "                                     'record_id': '9358af57-192f-4cc3-9bee-837e76819674'},\n",
      "                                   { 'dataset_id': '4e8ade31-7729-4891-a462-2dac66158512',\n",
      "                                     'json_metadata': {'units': 'ft^3/s'},\n",
      "                                     'name': 'Streamflow',\n",
      "                                     'record_id': 'c22deb3b-ebda-48cb-950a-2f4f00498197'},\n",
      "                                   { 'dataset_id': '4e8ade31-7729-4891-a462-2dac66158512',\n",
      "                                     'json_metadata': {'units': 'ft'},\n",
      "                                     'name': 'GageHeight',\n",
      "                                     'record_id': '499c54ed-3769-41ec-adac-30cf513c64e7'}]}}\n"
     ]
    }
   ],
   "source": [
    "# Again, these ids are optional and will be auto-generated if not given. They are included here in order\n",
    "# to make requests indempotent (so that new records aren't beeing generated every time this code block is run)\n",
    "\n",
    "time_variable_record_id = '9358af57-192f-4cc3-9bee-837e76819674'\n",
    "streamflow_variable_record_id = 'c22deb3b-ebda-48cb-950a-2f4f00498197'\n",
    "gageheight_variable_record_id = '499c54ed-3769-41ec-adac-30cf513c64e7'\n",
    "\n",
    "variable_defs = {\n",
    "    \"variables\": [\n",
    "        {\n",
    "            \"record_id\": time_variable_record_id, # If you remove this line, record_id will be auto-generated\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"name\": \"Time\",\n",
    "            \"metadata\": {\n",
    "                \"units\": \"ISO8601_datetime\"\n",
    "                # Can include any other metadata that you want to associate with the variable\n",
    "            },\n",
    "            \"standard_variable_ids\": [\n",
    "                # Recall that we created \"time_standard_variable\" python object after\n",
    "                # registering our standard variables. We just need its unique identifier - \n",
    "                # record_id - in order to associate it with our \"Time\" variable. Also, note \n",
    "                # that \"standard_variable_ids\" is an array, so you can associate multiple\n",
    "                # standard variables with our \"local\" variable (and it does not have\n",
    "                # to be done all at once). That is how we can semantically link multiple\n",
    "                # standard names and ontologies later on\n",
    "                time_standard_variable[\"record_id\"]\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": streamflow_variable_record_id, # If you remove this line, record_id will be auto-generated\n",
    "            \"dataset_id\": dataset_record_id, # from register_datasets() call\n",
    "            \"name\": \"Streamflow\",\n",
    "            \"metadata\": {\n",
    "                \"units\": \"ft^3/s\"\n",
    "            },\n",
    "            \"standard_variable_ids\": [\n",
    "                streamflow_standard_variable[\"record_id\"]\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": gageheight_variable_record_id, # If you remove this line, record_id will be auto-generated\n",
    "            \"dataset_id\": dataset_record_id, # from register_datasets() call\n",
    "            \"name\": \"GageHeight\",\n",
    "            \"metadata\": {\n",
    "                \"units\": \"ft\"\n",
    "            },\n",
    "            \"standard_variable_ids\": [\n",
    "                gageheight_standard_variable[\"record_id\"]\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/datasets/register_variables\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=variable_defs)\n",
    "\n",
    "parsed_response = handle_api_response(resp, print_response=True)\n",
    "variables = parsed_response[\"variables\"]\n",
    "\n",
    "time_variable = next(record for record in variables if record[\"name\"] == \"Time\")\n",
    "streamflow_variable = next(record for record in variables if record[\"name\"] == \"Streamflow\")\n",
    "gageheight_variable = next(record for record in variables if record[\"name\"] == \"GageHeight\")\n",
    "## Uncomment below to print individual records\n",
    "# print(f\"Time Variable: {time_variable}\")\n",
    "# print(f\"Temperature Variable: {temperature_variable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset specific: Clean up and split up data into files by location.\n",
    "The data files need to be put in csv format, and should be split up for better parsability and metadata annotation.\n",
    "The date format needs to be changed to ISO-8601 standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site found:  USGS06721500\n",
      "Site found:  USGS06724970\n",
      "Site found:  USGS06725450\n",
      "Site found:  USGS06726900\n",
      "Site found:  USGS06727000\n",
      "Site found:  USGS06727410\n",
      "Site found:  USGS06727500\n",
      "Site found:  USGS06730160\n",
      "Site found:  USGS06730200\n",
      "Site found:  USGS06730400\n",
      "Site found:  USGS06730500\n",
      "Site found:  USGS06730525\n",
      "Site found:  USGS395331105134400\n",
      "Site found:  USGS395452105113800\n"
     ]
    }
   ],
   "source": [
    "# The header separator separates general information at the top of the file (which is discarded) from the \n",
    "# information for each individual file concatenated in the overall file\n",
    "header_end = '# -----------------------------------------------------------------------------------\\n'\n",
    "# The first two lines of each site's file are a blank comment line followed by a line that starts with site_sep\n",
    "site_sep = '# Data provided for site '\n",
    "# Relative location of the input file\n",
    "ext = 'HAND-Data/streamgages/'\n",
    "\n",
    "# read in file and place in a list of lines:\n",
    "with open(ext+'USGS-Streamgages-HUC10190005.tsv') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# determine where the concatenated files start, discard the top header information\n",
    "start_index = lines.index(header_end) + 1\n",
    "# define the comment indicator\n",
    "comment = '#'\n",
    "# define the column names by a non-coded (human readable) name\n",
    "columns = ['Time','Streamflow','GageHeight']\n",
    "# first entry on the line with the column labels\n",
    "column_labels = 'agency_cd'\n",
    "# indicator for the start of a line of data\n",
    "dataline = 'USGS'\n",
    "\n",
    "# is a file currently open for writing?\n",
    "fopen = False\n",
    "# currently in a comment section?\n",
    "comment_section = True\n",
    "# translate line to current output file?\n",
    "writeline = False\n",
    "# skip the current line from writing to file?\n",
    "skip = False\n",
    "# for debugging: detected first data line after the column labels\n",
    "#one_data_line = False\n",
    "\n",
    "#iterate over the lines in the input file, starting past the main header\n",
    "for line in lines[start_index:]:\n",
    "    \n",
    "    # if this line should be skipped, set writeline to False and toggle skip\n",
    "    if skip:\n",
    "        writeline = False\n",
    "        skip = False\n",
    "        \n",
    "    # if current line is a coment line and not in a comment section at the moment\n",
    "    # then close the current write file (if open), and don't write the line to file\n",
    "    # (this is the empty comment line between concatenated files)\n",
    "    if line.startswith(comment) and not comment_section:\n",
    "        comment_section = True\n",
    "        if fopen:\n",
    "            f.close()\n",
    "            fopen = False\n",
    "        writeline = False\n",
    "    \n",
    "    # if current line is the first line in a new file, grab the name of the site\n",
    "    # and open a new file with the name of the site in the filename\n",
    "    if line.startswith(site_sep):\n",
    "        site_name = 'USGS' + line.split(site_sep)[1].strip()\n",
    "        print('Site found: ',site_name)\n",
    "        f = open(ext+site_name+'.csv','w')\n",
    "        fopen = True\n",
    "        writeline = True\n",
    "        \n",
    "    # if the current line is a comment line and currently in the comment section and\n",
    "    # a file is currently open, then write the comment to file\n",
    "    if line.startswith(comment) and comment_section and fopen:\n",
    "        writeline = True\n",
    "    \n",
    "    # if the current line does not start with a comment and we are in the comment section, \n",
    "    # then we have reached the column labels line; toggle comment_section\n",
    "    if not line.startswith(comment) and comment_section and fopen:\n",
    "        comment_section = False\n",
    "        writeline = True\n",
    "        \n",
    "    # If the current line starts with the column label indicator, then determine which columns are present\n",
    "    # and compile the appropriate column labels line to write to output file\n",
    "    if line.startswith(column_labels):\n",
    "        # for debigging -- toggle the indicator for the first line of data\n",
    "        #one_data_line = True\n",
    "        \n",
    "        # tab delimited data, so split along tabs\n",
    "        labels = line.split('\\t')\n",
    "        \n",
    "        # helper function to determine the index of a column in the list of column names\n",
    "        def find_index(labels, category):\n",
    "            index = -1\n",
    "            i = 0\n",
    "            for l in labels:\n",
    "                if l.endswith(category):\n",
    "                    index = i\n",
    "                    break\n",
    "                i += 1\n",
    "            return index\n",
    "        discharge_index = find_index(labels, '_00060')\n",
    "        gage_height_index = find_index(labels, '_00065')\n",
    "        \n",
    "        #print(discharge_index,gage_height_index)\n",
    "        # create the column labels line\n",
    "        if (discharge_index>=0) and (gage_height_index>=0):\n",
    "            line = ','.join(columns) + '\\n'\n",
    "        elif (discharge_index>=0):\n",
    "            line = ','.join(columns[0:2]) + '\\n'\n",
    "        else:\n",
    "            line = ','.join([columns[0],columns[1]]) + '\\n'\n",
    "            \n",
    "        writeline = True\n",
    "        # the line after the column labels line contains extra information that will not\n",
    "        # be written to output\n",
    "        skip = True\n",
    "        \n",
    "    # if the current line is a dataline, write it to file\n",
    "    if line.startswith(dataline):\n",
    "        writeline = True\n",
    "        #if one_data_line:\n",
    "        #    print(line)\n",
    "        #    one_data_line = False\n",
    "        \n",
    "    # if the current line is to be written to file, then write it\n",
    "    if writeline:\n",
    "        # if the line is a dataline, extract the date, streamflow and gageheight (as applicable); \n",
    "        # reformat date as ISO8601 YYYY-MM-DDThh:mm-MDT/MST offset (+)\n",
    "        if line.startswith(dataline):\n",
    "            components = line.split('\\t')\n",
    "            line = components[2].replace(' ','T')+'-'+components[3].replace('MDT','06').replace('MST','07')\n",
    "            if (discharge_index>=0):\n",
    "                line += ',' + components[discharge_index] \n",
    "            if (gage_height_index>=0):\n",
    "                line += ',' + components[gage_height_index] \n",
    "            line += '\\n'\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare data storage url and file names -- !! CURRENTLY NOT AVAILABLE SINCE MARIA DOES NOT HAVE ACCESS TO DATAX\n",
    "data_storage_url = \"www.my_domain.com/storage\"\n",
    "file_1_name = \"USGS06721500.csv\"\n",
    "file_2_name = \"USGS06724970.csv\"\n",
    "file_3_name = \"USGS06725450.csv\"\n",
    "file_4_name = \"USGS06726900.csv\"\n",
    "file_5_name = \"USGS06727000.csv\"\n",
    "file_6_name = \"USGS06727410.csv\"\n",
    "file_7_name = \"USGS06727500.csv\"\n",
    "file_8_name = \"USGS06730160.csv\"\n",
    "file_9_name = \"USGS06730200.csv\"\n",
    "file_10_name = \"USGS06730400.csv\"\n",
    "file_11_name = \"USGS06730500.csv\"\n",
    "file_12_name = \"USGS06730525.csv\"\n",
    "file_13_name = \"USGS395331105134400.csv\"\n",
    "file_14_name = \"USGS395452105113800.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare file urls\n",
    "file_1_data_url = f\"{data_storage_url}/{file_1_name}\"\n",
    "file_2_data_url = f\"{data_storage_url}/{file_2_name}\"\n",
    "file_3_data_url = f\"{data_storage_url}/{file_3_name}\"\n",
    "file_4_data_url = f\"{data_storage_url}/{file_4_name}\"\n",
    "file_5_data_url = f\"{data_storage_url}/{file_5_name}\"\n",
    "file_6_data_url = f\"{data_storage_url}/{file_6_name}\"\n",
    "file_7_data_url = f\"{data_storage_url}/{file_7_name}\"\n",
    "file_8_data_url = f\"{data_storage_url}/{file_8_name}\"\n",
    "file_9_data_url = f\"{data_storage_url}/{file_9_name}\"\n",
    "file_10_data_url = f\"{data_storage_url}/{file_10_name}\"\n",
    "file_11_data_url = f\"{data_storage_url}/{file_11_name}\"\n",
    "file_12_data_url = f\"{data_storage_url}/{file_12_name}\"\n",
    "file_13_data_url = f\"{data_storage_url}/{file_13_name}\"\n",
    "file_14_data_url = f\"{data_storage_url}/{file_14_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to dataset and variable registrations, we are going to generate unique resource record_ids to \n",
    "# make these requests repeatable without creating new records. But remember, these will be auto-generated\n",
    "# if not given\n",
    "\n",
    "#one time uuid generator\n",
    "#for i in range(2):\n",
    "#    print(str(uuid.uuid4()))\n",
    "    \n",
    "file_1_record_id = \"dd52e66b-3149-4d46-8f8e-a18e46136e55\"\n",
    "file_2_record_id = \"25916ccf-d108-4187-b243-2b257ce67fa5\"\n",
    "file_3_record_id = \"f72e52a0-5d80-474a-a35a-93a9836e72fa\"\n",
    "file_4_record_id = \"3bec0225-c63c-435b-b117-3a19dc86578e\"\n",
    "file_5_record_id = \"14c4d46c-c2c4-4aac-8683-343205eb8e7\"\n",
    "file_6_record_id = \"eabfa797-6eaf-4f8c-b11b-975a39fca51c\"\n",
    "file_7_record_id = \"479803fc-ffcb-4deb-9848-247a4bbf8d73\"\n",
    "file_8_record_id = \"97d97437-8b9c-462b-b0d6-0648228df842\"\n",
    "file_9_record_id = \"3471873a-74c3-45f7-9d91-c8038ee94ebe\"\n",
    "file_10_record_id = \"4795320b-37a5-4eb1-8c42-b83808862b04\"\n",
    "file_11_record_id = \"1be6de1a-653a-4b52-b02c-6f0ea1d3f487\"\n",
    "file_12_record_id = \"9ca76777-8839-459f-97bc-5eba9519522a\"\n",
    "file_13_record_id = \"7cfd0978-11f5-4da3-aad9-df8d98f677c6\"\n",
    "file_14_record_id = \"b60c897a-9b28-45d2-aa5c-467def5a318a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the temporal coverage in ISO format, UTC value conversion\n",
    "file_1_temporal_coverage = {\n",
    "    \"start_time\": \"1986-10-01T07:00:00\",\n",
    "    \"end_time\": \"2007-09-30T06:00:00\"\n",
    "}\n",
    "file_2_temporal_coverage = {\n",
    "    \"start_time\": \"2014-03-06T07:00:00\",\n",
    "    \"end_time\": \"2019-06-18T20:10:00\"\n",
    "}\n",
    "file_3_temporal_coverage = {\n",
    "    \"start_time\": \"1988-10-01T07:29:00\",\n",
    "    \"end_time\": \"2013-09-12T05:45:00\"\n",
    "}\n",
    "file_4_temporal_coverage = {\n",
    "    \"start_time\": \"1986-10-01T06:15:00\",\n",
    "    \"end_time\": \"1995-04-01T06:45:00\"\n",
    "}\n",
    "file_5_temporal_coverage = {\n",
    "    \"start_time\": \"1988-12-21T07:00:00\",\n",
    "    \"end_time\": \"1993-08-18T05:45:00\"\n",
    "}\n",
    "file_6_temporal_coverage = {\n",
    "    \"start_time\": \"2011-04-01T07:00:00\",\n",
    "    \"end_time\": \"2013-09-12T05:55:00\"\n",
    "}\n",
    "file_7_temporal_coverage = {\n",
    "    \"start_time\": \"1986-10-01T06:15:00\",\n",
    "    \"end_time\": \"2019-06-18T20:50:00\"\n",
    "}\n",
    "file_8_temporal_coverage = {\n",
    "    \"start_time\": \"2013-04-01T06:55:00\",\n",
    "    \"end_time\": \"2019-06-18T20:50:00\"\n",
    "}\n",
    "file_9_temporal_coverage = {\n",
    "    \"start_time\": \"1987-09-17T06:00:00\",\n",
    "    \"end_time\": \"2019-06-18T20:15:00\"\n",
    "}\n",
    "file_10_temporal_coverage = {\n",
    "    \"start_time\": \"1997-07-02T06:00:00\",\n",
    "    \"end_time\": \"2005-10-01T05:45:00\"\n",
    "}\n",
    "file_11_temporal_coverage = {\n",
    "    \"start_time\": \"1986-10-01T06:30:00\",\n",
    "    \"end_time\": \"2019-06-18T21:00:00\"\n",
    "}\n",
    "file_12_temporal_coverage = {\n",
    "    \"start_time\": \"2013-09-19T06:00:00\",\n",
    "    \"end_time\": \"2019-06-18T20:00:00\"\n",
    "}\n",
    "file_13_temporal_coverage = {\n",
    "    \"start_time\": \"1994-05-05T06:00:00\",\n",
    "    \"end_time\": \"1996-10-01T05:45:00\"\n",
    "}\n",
    "file_14_temporal_coverage = {\n",
    "    \"start_time\": \"1995-10-17T06:00:00\",\n",
    "    \"end_time\": \"1996-09-29T23:00:00\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add location coordinates pertaining to each file. The data catalog only accepts BoundingBox format at the moment, so setting a very small area with xmin, ymin = the coordinates provided by USGS. Coordinates converted to decimal and presented in WGS 84 (standard used in data catalog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert deg-min-sec to decimal\n",
    "#def(degminsec):\n",
    "    \n",
    "\n",
    "file_1_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.218889, \n",
    "        \"ymin\": -105.527778,\n",
    "        \"xmax\": 40.218890,\n",
    "        \"ymax\": -105.527779,\n",
    "    }\n",
    "}\n",
    "file_2_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.134278, \n",
    "        \"ymin\": -105.130819,\n",
    "        \"xmax\": 40.134279, \n",
    "        \"ymax\": -105.130820,\n",
    "    }\n",
    "}\n",
    "file_3_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.157422, \n",
    "        \"ymin\": -105.015394,\n",
    "        \"xmax\": 40.157423,\n",
    "        \"ymax\": -105.015395\n",
    "    }\n",
    "}\n",
    "file_4_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.011667, \n",
    "        \"ymin\": -105.348456,\n",
    "        \"xmax\": 40.011668,\n",
    "        \"ymax\": -105.348457\n",
    "    }\n",
    "}\n",
    "file_5_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.006389, \n",
    "        \"ymin\": -105.330278,\n",
    "        \"xmax\": 40.006390,\n",
    "        \"ymax\": -105.330279\n",
    "    }\n",
    "}\n",
    "file_6_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.042028, \n",
    "        \"ymin\": -105.364917,\n",
    "        \"xmax\": 40.042029,\n",
    "        \"ymax\": -105.364918\n",
    "    }\n",
    "}\n",
    "file_7_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.018667, \n",
    "        \"ymin\": -105.32625,\n",
    "        \"xmax\": 40.018668,\n",
    "        \"ymax\": -105.32626\n",
    "    }\n",
    "}\n",
    "file_8_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.057611, \n",
    "        \"ymin\": -105.348778,\n",
    "        \"xmax\": 40.057612,\n",
    "        \"ymax\": -105.348779\n",
    "    }\n",
    "}\n",
    "file_9_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.051667, \n",
    "        \"ymin\": -105.178333,\n",
    "        \"xmax\": 40.051668,\n",
    "        \"ymax\": -105.178334\n",
    "    }\n",
    "}\n",
    "file_10_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 39.976111, \n",
    "        \"ymin\": -105.116667,\n",
    "        \"xmax\": 39.976112,\n",
    "        \"ymax\": -105.116668\n",
    "    }\n",
    "}\n",
    "file_11_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.138778, \n",
    "        \"ymin\": -105.020222,\n",
    "        \"xmax\": 40.138779,\n",
    "        \"ymax\": -105.020223\n",
    "    }\n",
    "}\n",
    "file_12_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 40.160467, \n",
    "        \"ymin\": -105.007936,\n",
    "        \"xmax\": 40.160468,\n",
    "        \"ymax\": -105.007937\n",
    "    }\n",
    "}\n",
    "file_13_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 39.891944, \n",
    "        \"ymin\": -105.228889,\n",
    "        \"xmax\": 39.891945,\n",
    "        \"ymax\": -105.228890\n",
    "    }\n",
    "}\n",
    "file_14_spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 39.915833, \n",
    "        \"ymin\": -105.193611,\n",
    "        \"xmax\": 39.915834,\n",
    "        \"ymax\": -105.193612\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the actual registration request. Inspect all information before official submission!! Need a way to set header char to '#'\n",
    "Data files need to have a URL, so below yields an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'API Response': { 'error': \"{'VariableSchemaValidationError': [{'record': \"\n",
      "                             \"{'dataset_id': \"\n",
      "                             \"'4e8ade31-7729-4891-a462-2dac66158512', \"\n",
      "                             \"'provenance_id': \"\n",
      "                             \"'e8287ea4-e6f2-47aa-8bfc-0c22852735c8', 'name': \"\n",
      "                             \"'USGS06727000.csv', 'record_id': \"\n",
      "                             \"'14c4d46c-c2c4-4aac-8683-343205eb8e7', \"\n",
      "                             \"'variable_ids': \"\n",
      "                             \"['9358af57-192f-4cc3-9bee-837e76819674', \"\n",
      "                             \"'c22deb3b-ebda-48cb-950a-2f4f00498197'], \"\n",
      "                             \"'resource_type': 'csv', 'data_url': \"\n",
      "                             \"'www.my_domain.com/storage/USGS06727000.csv', \"\n",
      "                             \"'json_metadata': {'spatial_coverage': {'type': \"\n",
      "                             \"'BoundingBox', 'value': {'xmin': 40.006389, \"\n",
      "                             \"'ymin': -105.330278, 'xmax': 40.00639, 'ymax': \"\n",
      "                             \"-105.330279}}, 'temporal_coverage': \"\n",
      "                             \"{'start_time': '1988-12-21T07:00:00', \"\n",
      "                             \"'end_time': '1993-08-18T05:45:00'}, \"\n",
      "                             \"'geospatial_metadata': {'srs': {'srid': \"\n",
      "                             \"'EPSG:4267'}}, 'delimiter': ',', 'has_header': \"\n",
      "                             \"True}, 'layout': {}, 'temporal_coverage': \"\n",
      "                             \"{'start_time': '1988-12-21T07:00:00', \"\n",
      "                             \"'end_time': '1993-08-18T05:45:00'}, \"\n",
      "                             \"'spatial_coverage': {'type': 'BoundingBox', \"\n",
      "                             \"'value': {'xmin': 40.006389, 'ymin': \"\n",
      "                             \"-105.330278, 'xmax': 40.00639, 'ymax': \"\n",
      "                             \"-105.330279}}}, 'errors': ['record_id must be a \"\n",
      "                             'valid UUID v4; received '\n",
      "                             \"14c4d46c-c2c4-4aac-8683-343205eb8e7']}]}\"}}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Bad request ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f1ad7ff539a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m \u001b[0mparsed_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_api_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-543749d2427a>\u001b[0m in \u001b[0;36mhandle_api_response\u001b[0;34m(response, print_response)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bad request ^\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Please make sure your request headers include X-Api-Key and that you are using correct url\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Bad request ^"
     ]
    }
   ],
   "source": [
    "# submit register request\n",
    "resource_defs = {\n",
    "    \"resources\": [\n",
    "        {\n",
    "            \"record_id\": file_1_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_1_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_1_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_1_spatial_coverage,\n",
    "                \"temporal_coverage\": file_1_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4267\" #NAD27 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_2_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"],\n",
    "                gageheight_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_2_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_2_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_2_spatial_coverage,\n",
    "                \"temporal_coverage\": file_2_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4269\" #NAD83 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_3_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_3_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_3_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_3_spatial_coverage,\n",
    "                \"temporal_coverage\": file_3_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4269\" #NAD83 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_4_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_4_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_4_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_4_spatial_coverage,\n",
    "                \"temporal_coverage\": file_4_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4267\" #NAD27 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_5_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_5_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_5_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_5_spatial_coverage,\n",
    "                \"temporal_coverage\": file_5_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4267\" #NAD27 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_6_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_6_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_6_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_6_spatial_coverage,\n",
    "                \"temporal_coverage\": file_6_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4269\" #NAD83 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_7_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"],\n",
    "                gageheight_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_7_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_7_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_7_spatial_coverage,\n",
    "                \"temporal_coverage\": file_7_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4269\" #NAD83 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_8_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"],\n",
    "                gageheight_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_8_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_8_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_8_spatial_coverage,\n",
    "                \"temporal_coverage\": file_8_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4269\" #NAD83 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_9_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"],\n",
    "                gageheight_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_9_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_9_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_9_spatial_coverage,\n",
    "                \"temporal_coverage\": file_9_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4267\" #NAD27 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_10_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_10_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_10_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_10_spatial_coverage,\n",
    "                \"temporal_coverage\": file_10_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4267\" #NAD27 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_11_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"],\n",
    "                gageheight_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_11_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_11_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_11_spatial_coverage,\n",
    "                \"temporal_coverage\": file_11_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4269\" #NAD83 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_12_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"],\n",
    "                gageheight_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_12_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_12_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_12_spatial_coverage,\n",
    "                \"temporal_coverage\": file_12_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4269\" #NAD83 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_13_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_13_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_13_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_13_spatial_coverage,\n",
    "                \"temporal_coverage\": file_13_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4267\" #NAD27 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_14_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                streamflow_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_14_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_14_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": file_14_spatial_coverage,\n",
    "                \"temporal_coverage\": file_14_temporal_coverage,\n",
    "                \"geospatial_metadata\": {\n",
    "                    \"srs\": {\n",
    "                        \"srid\": \"EPSG:4267\" #NAD27 projection\n",
    "                    },\n",
    "                },\n",
    "                \"delimiter\": \",\",\n",
    "                \"has_header\": True,\n",
    "                #header_char? ... '#'\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ... and register them in bulk\n",
    "resp = requests.post(f\"{url}/datasets/register_resources\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=resource_defs)\n",
    "\n",
    "\n",
    "parsed_response = handle_api_response(resp, print_response=True)\n",
    "\n",
    "\n",
    "resources = parsed_response[\"resources\"]\n",
    "    \n",
    "resource_1 = next(record for record in resources if record[\"name\"] == file_1_name)\n",
    "resource_2 = next(record for record in resources if record[\"name\"] == file_2_name)\n",
    "resource_3 = next(record for record in resources if record[\"name\"] == file_3_name)\n",
    "resource_4 = next(record for record in resources if record[\"name\"] == file_4_name)\n",
    "resource_5 = next(record for record in resources if record[\"name\"] == file_5_name)\n",
    "resource_6 = next(record for record in resources if record[\"name\"] == file_6_name)\n",
    "resource_7 = next(record for record in resources if record[\"name\"] == file_7_name)\n",
    "resource_8 = next(record for record in resources if record[\"name\"] == file_8_name)\n",
    "resource_9 = next(record for record in resources if record[\"name\"] == file_9_name)\n",
    "resource_10 = next(record for record in resources if record[\"name\"] == file_10_name)\n",
    "resource_11 = next(record for record in resources if record[\"name\"] == file_11_name)\n",
    "resource_12 = next(record for record in resources if record[\"name\"] == file_12_name)\n",
    "resource_13 = next(record for record in resources if record[\"name\"] == file_13_name)\n",
    "resource_14 = next(record for record in resources if record[\"name\"] == file_14_name)\n",
    "\n",
    "## Uncomment below to print individual records    \n",
    "# print(f\"{file_1_name}: {resource_1}\")\n",
    "# print(f\"{file_2_name}: {resource_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
